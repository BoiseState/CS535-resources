23/07/30 22:55:05 INFO SparkContext: Running Spark version 3.4.1
23/07/30 22:55:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/07/30 22:55:05 INFO ResourceUtils: ==============================================================
23/07/30 22:55:05 INFO ResourceUtils: No custom resources configured for spark.driver.
23/07/30 22:55:05 INFO ResourceUtils: ==============================================================
23/07/30 22:55:05 INFO SparkContext: Submitted application: CaseAnalysis
23/07/30 22:55:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/07/30 22:55:05 INFO ResourceProfile: Limiting resource is cpu
23/07/30 22:55:05 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/07/30 22:55:05 INFO SecurityManager: Changing view acls to: amit
23/07/30 22:55:05 INFO SecurityManager: Changing modify acls to: amit
23/07/30 22:55:05 INFO SecurityManager: Changing view acls groups to: 
23/07/30 22:55:05 INFO SecurityManager: Changing modify acls groups to: 
23/07/30 22:55:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: amit; groups with view permissions: EMPTY; users with modify permissions: amit; groups with modify permissions: EMPTY
23/07/30 22:55:05 INFO Utils: Successfully started service 'sparkDriver' on port 42279.
23/07/30 22:55:05 INFO SparkEnv: Registering MapOutputTracker
23/07/30 22:55:05 INFO SparkEnv: Registering BlockManagerMaster
23/07/30 22:55:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/07/30 22:55:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/07/30 22:55:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/07/30 22:55:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5f201c18-e3c5-4d9c-aed6-b963a3b2de37
23/07/30 22:55:05 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/07/30 22:55:05 INFO SparkEnv: Registering OutputCommitCoordinator
23/07/30 22:55:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/07/30 22:55:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/07/30 22:55:06 INFO SparkContext: Added JAR file:/home/amit/Documents/CS535-resources/examples/Spark/case-analysis/case-analysis.jar at spark://fedora:42279/jars/case-analysis.jar with timestamp 1690779305587
23/07/30 22:55:06 INFO Executor: Starting executor ID driver on host fedora
23/07/30 22:55:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/07/30 22:55:06 INFO Executor: Fetching spark://fedora:42279/jars/case-analysis.jar with timestamp 1690779305587
23/07/30 22:55:06 INFO TransportClientFactory: Successfully created connection to fedora/10.211.55.3:42279 after 17 ms (0 ms spent in bootstraps)
23/07/30 22:55:06 INFO Utils: Fetching spark://fedora:42279/jars/case-analysis.jar to /tmp/spark-99843dc0-98f9-43b7-b731-cd5246a56c73/userFiles-0272e1a5-9910-4e64-a54f-0d35d3e60820/fetchFileTemp11048584912832484484.tmp
23/07/30 22:55:06 INFO Executor: Adding file:/tmp/spark-99843dc0-98f9-43b7-b731-cd5246a56c73/userFiles-0272e1a5-9910-4e64-a54f-0d35d3e60820/case-analysis.jar to class loader
23/07/30 22:55:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34085.
23/07/30 22:55:06 INFO NettyBlockTransferService: Server created on fedora:34085
23/07/30 22:55:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/07/30 22:55:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fedora, 34085, None)
23/07/30 22:55:06 INFO BlockManagerMasterEndpoint: Registering block manager fedora:34085 with 434.4 MiB RAM, BlockManagerId(driver, fedora, 34085, None)
23/07/30 22:55:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fedora, 34085, None)
23/07/30 22:55:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fedora, 34085, None)
23/07/30 22:55:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)
23/07/30 22:55:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)
23/07/30 22:55:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fedora:34085 (size: 32.6 KiB, free: 434.4 MiB)
23/07/30 22:55:06 INFO SparkContext: Created broadcast 0 from textFile at CaseAnalysis.java:28
23/07/30 22:55:06 INFO FileInputFormat: Total input files to process : 14
23/07/30 22:55:06 INFO SparkContext: Starting job: count at CaseAnalysis.java:30
23/07/30 22:55:06 INFO DAGScheduler: Got job 0 (count at CaseAnalysis.java:30) with 14 output partitions
23/07/30 22:55:06 INFO DAGScheduler: Final stage: ResultStage 0 (count at CaseAnalysis.java:30)
23/07/30 22:55:06 INFO DAGScheduler: Parents of final stage: List()
23/07/30 22:55:06 INFO DAGScheduler: Missing parents: List()
23/07/30 22:55:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at flatMap at CaseAnalysis.java:29), which has no missing parents
23/07/30 22:55:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.8 KiB, free 434.1 MiB)
23/07/30 22:55:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 434.1 MiB)
23/07/30 22:55:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fedora:34085 (size: 3.2 KiB, free: 434.4 MiB)
23/07/30 22:55:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
23/07/30 22:55:06 INFO DAGScheduler: Submitting 14 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at flatMap at CaseAnalysis.java:29) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13))
23/07/30 22:55:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 14 tasks resource profile 0
23/07/30 22:55:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (fedora, executor driver, partition 0, PROCESS_LOCAL, 7479 bytes) 
23/07/30 22:55:06 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (fedora, executor driver, partition 1, PROCESS_LOCAL, 7474 bytes) 
23/07/30 22:55:06 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (fedora, executor driver, partition 2, PROCESS_LOCAL, 7480 bytes) 
23/07/30 22:55:06 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (fedora, executor driver, partition 3, PROCESS_LOCAL, 7475 bytes) 
23/07/30 22:55:06 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
23/07/30 22:55:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/07/30 22:55:06 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
23/07/30 22:55:06 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
23/07/30 22:55:06 INFO HadoopRDD: Input split: file:/home/amit/Documents/CS535-resources/examples/Spark/case-analysis/input/Alice-in-Wonderland.txt:0+158315
23/07/30 22:55:06 INFO HadoopRDD: Input split: file:/home/amit/Documents/CS535-resources/examples/Spark/case-analysis/input/Bill-of-Rights.txt:0+10641
23/07/30 22:55:06 INFO HadoopRDD: Input split: file:/home/amit/Documents/CS535-resources/examples/Spark/case-analysis/input/Complete-Shakespeare.txt:0+5458248
23/07/30 22:55:06 INFO HadoopRDD: Input split: file:/home/amit/Documents/CS535-resources/examples/Spark/case-analysis/input/Decl-of-Ind-USA.txt:0+15890
23/07/30 22:55:06 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1016 bytes result sent to driver
23/07/30 22:55:06 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 973 bytes result sent to driver
23/07/30 22:55:06 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 973 bytes result sent to driver
23/07/30 22:55:06 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (fedora, executor driver, partition 4, PROCESS_LOCAL, 7473 bytes) 
23/07/30 22:55:06 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
23/07/30 22:55:06 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (fedora, executor driver, partition 5, PROCESS_LOCAL, 7468 bytes) 
23/07/30 22:55:06 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (fedora, executor driver, partition 6, PROCESS_LOCAL, 7478 bytes) 
23/07/30 22:55:06 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
23/07/30 22:55:06 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
23/07/30 22:55:06 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 175 ms on fedora (executor driver) (1/14)
23/07/30 22:55:06 INFO HadoopRDD: Input split: file:/home/amit/Documents/CS535-resources/examples/Spark/case-analysis/input/Encyclopaedia.txt:0+8441343
23/07/30 22:55:06 INFO HadoopRDD: Input split: file:/home/amit/Documents/CS535-resources/examples/Spark/case-analysis/input/Flatland.txt:0+203918
23/07/30 22:55:06 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 181 ms on fedora (executor driver) (2/14)
23/07/30 22:55:06 INFO HadoopRDD: Input split: file:/home/amit/Documents/CS535-resources/examples/Spark/case-analysis/input/Gettysburg-Address.txt:0+1688
23/07/30 22:55:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 207 ms on fedora (executor driver) (3/14)
23/07/30 22:55:06 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 973 bytes result sent to driver
23/07/30 22:55:06 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (fedora, executor driver, partition 7, PROCESS_LOCAL, 7476 bytes) 
23/07/30 22:55:06 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
23/07/30 22:55:06 INFO HadoopRDD: Input split: file:/home/amit/Documents/CS535-resources/examples/Spark/case-analysis/input/Gift-of-the-Magi.txt:0+21421
23/07/30 22:55:06 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 36 ms on fedora (executor driver) (4/14)
23/07/30 22:55:07 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 973 bytes result sent to driver
23/07/30 22:55:07 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (fedora, executor driver, partition 8, PROCESS_LOCAL, 7474 bytes) 
23/07/30 22:55:07 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
23/07/30 22:55:07 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 21 ms on fedora (executor driver) (5/14)
23/07/30 22:55:07 INFO HadoopRDD: Input split: file:/home/amit/Documents/CS535-resources/examples/Spark/case-analysis/input/Les-Miserables.txt:0+3263187
23/07/30 22:55:07 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1016 bytes result sent to driver
23/07/30 22:55:07 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (fedora, executor driver, partition 9, PROCESS_LOCAL, 7473 bytes) 
23/07/30 22:55:07 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
23/07/30 22:55:07 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 79 ms on fedora (executor driver) (6/14)
23/07/30 22:55:07 INFO HadoopRDD: Input split: file:/home/amit/Documents/CS535-resources/examples/Spark/case-analysis/input/Patrick-Henry.txt:0+14678
23/07/30 22:55:07 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 973 bytes result sent to driver
23/07/30 22:55:07 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (fedora, executor driver, partition 10, PROCESS_LOCAL, 7474 bytes) 
23/07/30 22:55:07 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 14 ms on fedora (executor driver) (7/14)
23/07/30 22:55:07 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
23/07/30 22:55:07 INFO HadoopRDD: Input split: file:/home/amit/Documents/CS535-resources/examples/Spark/case-analysis/input/Scarlet-Letter.txt:0+517303
23/07/30 22:55:07 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 1016 bytes result sent to driver
23/07/30 22:55:07 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (fedora, executor driver, partition 11, PROCESS_LOCAL, 7485 bytes) 
23/07/30 22:55:07 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 105 ms on fedora (executor driver) (8/14)
23/07/30 22:55:07 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
23/07/30 22:55:07 INFO HadoopRDD: Input split: file:/home/amit/Documents/CS535-resources/examples/Spark/case-analysis/input/Through-the-Looking-Glass.txt:0+178845
23/07/30 22:55:07 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 973 bytes result sent to driver
23/07/30 22:55:07 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (fedora, executor driver, partition 12, PROCESS_LOCAL, 7477 bytes) 
23/07/30 22:55:07 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 27 ms on fedora (executor driver) (9/14)
23/07/30 22:55:07 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
23/07/30 22:55:07 INFO HadoopRDD: Input split: file:/home/amit/Documents/CS535-resources/examples/Spark/case-analysis/input/Tom-Sawyer-Abroad.txt:0+183972
23/07/30 22:55:07 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 1016 bytes result sent to driver
23/07/30 22:55:07 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (fedora, executor driver, partition 13, PROCESS_LOCAL, 7475 bytes) 
23/07/30 22:55:07 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 40 ms on fedora (executor driver) (10/14)
23/07/30 22:55:07 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
23/07/30 22:55:07 INFO HadoopRDD: Input split: file:/home/amit/Documents/CS535-resources/examples/Spark/case-analysis/input/US-Constitution.txt:0+34553
23/07/30 22:55:07 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 973 bytes result sent to driver
23/07/30 22:55:07 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 21 ms on fedora (executor driver) (11/14)
23/07/30 22:55:07 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 1016 bytes result sent to driver
23/07/30 22:55:07 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 291 ms on fedora (executor driver) (12/14)
23/07/30 22:55:07 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1016 bytes result sent to driver
23/07/30 22:55:07 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 585 ms on fedora (executor driver) (13/14)
23/07/30 22:55:07 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1016 bytes result sent to driver
23/07/30 22:55:07 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 526 ms on fedora (executor driver) (14/14)
23/07/30 22:55:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/07/30 22:55:07 INFO DAGScheduler: ResultStage 0 (count at CaseAnalysis.java:30) finished in 0.740 s
23/07/30 22:55:07 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/07/30 22:55:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/07/30 22:55:07 INFO DAGScheduler: Job 0 finished: count at CaseAnalysis.java:30, took 0.766007 s
Exception in thread "main" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/amit/Documents/CS535-resources/examples/Spark/case-analysis/output1 already exists
	at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)
	at org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:299)
	at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)
	at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)
	at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)
	at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)
	at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)
	at org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1593)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1593)
	at org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1579)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1579)
	at org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)
	at org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)
	at org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)
	at CaseAnalysis.main(CaseAnalysis.java:47)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1020)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
23/07/30 22:55:07 INFO SparkContext: Invoking stop() from shutdown hook
23/07/30 22:55:07 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/07/30 22:55:07 INFO SparkUI: Stopped Spark web UI at http://fedora:4040
23/07/30 22:55:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/07/30 22:55:07 INFO MemoryStore: MemoryStore cleared
23/07/30 22:55:07 INFO BlockManager: BlockManager stopped
23/07/30 22:55:07 INFO BlockManagerMaster: BlockManagerMaster stopped
23/07/30 22:55:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/07/30 22:55:07 INFO SparkContext: Successfully stopped SparkContext
23/07/30 22:55:07 INFO ShutdownHookManager: Shutdown hook called
23/07/30 22:55:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-c0da92d2-26c1-45f5-9c5a-366797cc11f0
23/07/30 22:55:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-99843dc0-98f9-43b7-b731-cd5246a56c73
